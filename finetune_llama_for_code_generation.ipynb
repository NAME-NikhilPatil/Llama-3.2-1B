{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuning Llama 3.2 for Python Code Generation\n\n",
        "This notebook walks through the process of fine-tuning the `meta-llama/Llama-3.2-1B-Instruct` model on a dataset of Python code instructions. The key steps are:\n\n",
        "1.  **Setup**: Installing dependencies and connecting to Google Drive.\n",
        "2.  **Data Preparation**: Loading and formatting the `iamtarun/python_code_instructions_18k_alpaca` dataset.\n",
        "3.  **Model Preparation**: Loading the base model with 4-bit quantization and configuring LoRA for efficient fine-tuning.\n",
        "4.  **Training**: Running the fine-tuning process using the `SFTTrainer`.\n",
        "5.  **Inference**: Loading the fine-tuned adapter and generating code from a sample prompt.\n",
        "6.  **Evaluation**: Evaluating the model's performance on the HumanEval benchmark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n\n",
        "First, we install the necessary libraries from our `requirements.txt` file. We also mount Google Drive to save model checkpoints during training, which is crucial for long-running jobs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hugging Face Login\n\n",
        "To download the Llama 3.2 model, you need to be logged into your Hugging Face account. We use a `config.json` file to store the access token securely. Make sure you have created this file from the `config.json.template`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "import json\n",
        "\n",
        "with open(\"config.json\", \"r\") as config_file:\n",
        "    config = json.load(config_file)\n",
        "    access_token = config[\"HF_ACCESS_TOKEN\"]\n",
        "\n",
        "login(token=access_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preparation\n\n",
        "We load the `iamtarun/python_code_instructions_18k_alpaca` dataset, which contains instructions and corresponding Python code. We then format the data into a prompt structure suitable for instruction fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from datasets.arrow_dataset import Dataset\n",
        "\n",
        "def format_sample(sample):\n",
        "    \"\"\" Helper function to format a single input sample\"\"\"\n",
        "    instruction=sample['instruction']\n",
        "    input_text=sample['input']\n",
        "    output_text=sample['output']\n",
        "\n",
        "    if input_text is None or input_text==\"\":\n",
        "        formatted_prompt=(\n",
        "            f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
        "            f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n",
        "            f\"### Instruction:\\n{instruction}\\n\\n\"\n",
        "            f\"### Response:\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "            f\"{output_text}<|eot_id|>\"\n",
        "        )\n",
        "    else:\n",
        "        formatted_prompt=(\n",
        "            f\"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
        "            f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n",
        "            f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n\"\n",
        "            f\"### Response:\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "            f\"{output_text}<|eot_id|>\"\n",
        "        )\n",
        "    formatted_prompt=\"\".join(formatted_prompt) # exclude trailing white spaces\n",
        "    return formatted_prompt                    # stream text into the dataloader, one by one\n",
        "\n",
        "\n",
        "\n",
        "def gen_train_input():\n",
        "    \"\"\" Format all data input in alpaca style\n",
        "        Return:\n",
        "            A generator on train data \"train_gen\"\n",
        "    \"\"\"\n",
        "    # load data\n",
        "    ds=load_dataset(\"iamtarun/python_code_instructions_18k_alpaca\",streaming=True, split=\"train\")\n",
        "    # datata set has 18.6k samples, we use 16.8k (90%) for training + 1.8k for validation\n",
        "    num_samples=16800\n",
        "    counter=0\n",
        "    for sample in iter(ds):\n",
        "        if counter>=num_samples:\n",
        "            break\n",
        "        formatted_prompt=format_sample(sample)\n",
        "        yield {'text': formatted_prompt}\n",
        "        counter+=1\n",
        "\n",
        "\n",
        "def gen_val_input():\n",
        "    \"\"\" Format all data input in alpaca style\n",
        "        Return:\n",
        "            A generator on val data \"val_gen\"\n",
        "    \"\"\"\n",
        "    # load data\n",
        "    ds=load_dataset(\"iamtarun/python_code_instructions_18k_alpaca\",streaming=True, split=\"train\")\n",
        "    # datata set has 18.6k samples, we use 16.8k (90%) for training + 1.8k for validation\n",
        "    num_samples=16800\n",
        "    counter=0\n",
        "    for sample in iter(ds):\n",
        "        if counter<num_samples:\n",
        "            counter+=1\n",
        "            continue\n",
        "\n",
        "        formatted_prompt=format_sample(sample)\n",
        "        yield {'text': formatted_prompt}\n",
        "        counter+=1\n",
        "\n",
        "dataset_train = Dataset.from_generator(gen_train_input)\n",
        "dataset_val=Dataset.from_generator(gen_val_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Train dataset size: {len(dataset_train)}\")\n",
        "print(f\"Validation dataset size: {len(dataset_val)}\")\n",
        "\n",
        "print(f\"Sample train:\\n{dataset_train[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Preparation\n\n",
        "We load the `meta-llama/Llama-3.2-1B-Instruct` model and its tokenizer. To make fine-tuning efficient, we apply two key techniques:\n",
        "- **4-bit Quantization**: We use `BitsAndBytesConfig` to load the model in 4-bit precision, significantly reducing its memory footprint.\n",
        "- **LoRA (Low-Rank Adaptation)**: We use `LoraConfig` from the PEFT library to inject trainable low-rank matrices into the model. This allows us to update only a small fraction of the model's parameters, making training much faster and less memory-intensive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import LoraConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\".*padding_side` right should be used.*\")\n",
        "\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "def create_and_prepare_model(hf_token=None):\n",
        "    \"\"\"Loads and prepares the quantized model and tokenizer for Colab GPU.\"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        raise SystemExit(\"GPU not found. This notebook requires a GPU.\")\n",
        "    print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "    compute_dtype = torch.bfloat16\n",
        "    print(f\"Using compute dtype: {compute_dtype}\")\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=compute_dtype,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "    print(f\"Loading model: {model_name} with 4-bit quantization...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        torch_dtype=compute_dtype,\n",
        "        device_map=\"auto\",\n",
        "        token=hf_token\n",
        "    )\n",
        "    print(\"Model loaded successfully.\")\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        lora_alpha=16,\n",
        "        r=16,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n",
        "    )\n",
        "    print(\"LoRA config created.\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "    print(f\"Tokenizer loaded and configured.\")\n",
        "\n",
        "    return model, peft_config, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Training the Model\n\n",
        "Now we define the training arguments and initialize the `SFTTrainer` from the TRL library. The trainer handles the entire training loop, including checkpointing, logging, and optimization.\n\n",
        "We configure the trainer to save checkpoints to Google Drive every 15 steps. This ensures that we don't lose progress if the Colab session disconnects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "import os\n",
        "\n",
        "output_dir_gdrive = \"/content/drive/MyDrive/colab_training/llama32-python-save15steps\"\n",
        "os.makedirs(output_dir_gdrive, exist_ok=True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir_gdrive,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=8,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    logging_steps=15,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=15,\n",
        "    save_total_limit=3,\n",
        "    learning_rate=2e-4,\n",
        "    bf16=True,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_ratio=0.03,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    report_to=\"tensorboard\",\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "model, peft_config, tokenizer = create_and_prepare_model(access_token)\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_train,\n",
        "    eval_dataset=dataset_val,\n",
        "    peft_config=peft_config,\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "trainer.train(resume_from_checkpoint=True)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "final_save_path = os.path.join(output_dir_gdrive, \"final_adapter\")\n",
        "trainer.save_model(final_save_path)\n",
        "print(f\"Final adapter model saved to: {final_save_path}\")"
      ]
    },
    {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## 5. Inference\n\n",
            "After training, we can load the fine-tuned model to perform inference. We load the base model in 4-bit and then apply the trained LoRA adapters on top."
        ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "def load_quantized_lora_model(base_model_id, adapter_directory, hf_token=None):\n",
        "    \"\"\"Loads the base model with 4-bit quantization and then applies the LoRA adapter.\"\"\"\n",
        "    print(f\"Loading base model: {base_model_id}...\")\n",
        "    model_dtype = torch.bfloat16\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=model_dtype,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "    \n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_id,\n",
        "        quantization_config=quantization_config,\n",
        "        torch_dtype=model_dtype,\n",
        "        device_map=\"auto\",\n",
        "        token=hf_token,\n",
        "    )\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_id, token=hf_token)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "    \n",
        "    print(f\"Loading LoRA adapters from: {adapter_directory}\")\n",
        "    model_with_adapters = PeftModel.from_pretrained(base_model, adapter_directory)\n",
        "    model_with_adapters.eval()\n",
        "    return model_with_adapters, tokenizer\n",
        "\n",
        "base_model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "adapter_directory = \"/content/drive/MyDrive/colab_training/llama32-python-save15steps/final_adapter\"\n",
        "\n",
        "model_ft, tokenizer = load_quantized_lora_model(base_model_id, adapter_directory, access_token)\n",
        "\n",
        "if model_ft is not None:\n",
        "    print(\"Fine-tuned model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate Code with a Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_with_hf(model, tokenizer, prompt, max_new_tokens=256, temperature=0.6, top_k=50, top_p=0.9):\n",
        "    model_device = next(model.parameters()).device\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    \n",
        "    try:\n",
        "        inputs_dict = tokenizer.apply_chat_template(\n",
        "            messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n",
        "        )\n",
        "        input_ids = inputs_dict['input_ids'].to(model_device)\n",
        "        attention_mask = inputs_dict.get('attention_mask')\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = attention_mask.to(model_device)\n",
        "    except Exception:\n",
        "        formatted_prompt = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "        inputs_dict = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
        "        input_ids = inputs_dict['input_ids'].to(model_device)\n",
        "        attention_mask = inputs_dict.get('attention_mask')\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = attention_mask.to(model_device)\n",
        "\n",
        "    input_length = input_ids.shape[1]\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        generate_kwargs = {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"max_new_tokens\": max_new_tokens,\n",
        "            \"eos_token_id\": [tokenizer.eos_token_id, 128009],\n",
        "            \"do_sample\": True,\n",
        "            \"temperature\": temperature,\n",
        "            \"top_k\": top_k,\n",
        "            \"top_p\": top_p,\n",
        "            \"pad_token_id\": tokenizer.pad_token_id\n",
        "        }\n",
        "        if attention_mask is not None:\n",
        "            generate_kwargs[\"attention_mask\"] = attention_mask\n",
        "\n",
        "        outputs = model.generate(**generate_kwargs)\n",
        "    \n",
        "    generated_ids = outputs[0, input_length:]\n",
        "    return tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "user_prompt = \"Write a Python function to calculate the factorial of a number.\"\n",
        "print(f\"User Prompt:\\n{user_prompt}\\n\")\n",
        "response = generate_with_hf(model_ft, tokenizer, user_prompt, max_new_tokens=150, temperature=0.2)\n",
        "print(f\"Generated Response:\\n-------------------\\n{response}\\n-------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluation on HumanEval\n\n",
        "To quantitatively assess the model's performance, we evaluate it on the HumanEval dataset. This benchmark consists of 164 programming problems with unit tests. We generate multiple code samples (`pass@10`) for each problem and use the `code_eval` metric to check for functional correctness."
      ]
    },
    {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
            "from datasets import load_dataset\n\n",
            "human_eval_dataset = load_dataset(\"openai_humaneval\")\n",
            "print(human_eval_dataset['test'][0]['prompt'])"
        ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n\n",
        "generated_code_samples = []\n",
        "for problem in tqdm(human_eval_dataset['test']):\n",
        "    prompt = problem['prompt']\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False).to(model_ft.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model_ft.generate(\n",
        "            **inputs,\n",
        "            num_return_sequences=10,\n",
        "            max_new_tokens=256,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.95,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    completions_only = [output.replace(prompt, \"\") for output in decoded_outputs]\n",
        "    generated_code_samples.append(completions_only)\n",
        "\n",
        "print(\"HumanEval generation complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import evaluate\n\n",
        "os.environ[\"HF_ALLOW_CODE_EVAL\"] = \"1\"\n",
        "code_eval = evaluate.load(\"code_eval\")\n",
        "\n",
        "test_cases = [problem[\"test\"] for problem in human_eval_dataset['test']]\n",
        "\n",
        "pass_at_k, results = code_eval.compute(\n",
        "    references=test_cases,\n",
        "    predictions=generated_code_samples,\n",
        "    k=[1, 10]\n",
        ")\n",
        "\n",
        "print(\"\\n--- Evaluation Complete ---\")\n",
        "print(pass_at_k)\n",
        "print(\"----------------------------\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}